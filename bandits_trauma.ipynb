{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessing the best EMT response using Bandit Algorithms\n",
    "\n",
    "A [controversial experiment is being performed in Philadelphia](http://www.npr.org/sections/health-shots/2016/10/20/496828573/will-a-study-save-victims-of-violence-or-gamble-with-their-lives). Researchers are exploring the appropriate response for a gunshot or stab victim: whether the victim receives advanced care at the scene or if they receive only basic care before being transported to a nearby hospital. Dr. Zoe Maher, trauma surgeon, mother, and a researcher conducting this study, asserts that it is better for victims to receive basic care until they reach the hospital. Her colleagues and she believe that advanced care, such as IV fluids\n",
    "and breathing tubes, will disrupt the body's natural measures of preventing blood\n",
    "like the constriction of blood vessels and the forming of clots.\n",
    "\n",
    "Most of the controversy from this study, at least from what the article describes, is due to how the study is conducted. Depending on the dispatch number of the city paramedics...\n",
    "\n",
    "* odd numbers will carry out advanced care\n",
    "* even numbers will provide basic care\n",
    "\n",
    "Just focusing on appearances, the words \"advanced\" and \"basic\" imply that half of the\n",
    "future stab and gunshot victims in Philadelphia will just randomly get sub-bar\n",
    "care (because \"advanced\" is better than \"basic\", right?). It's reasonable to see\n",
    "how the population may be worried.\n",
    "\n",
    "I want to do a basic intro to bandit algorithms and why they're better for this\n",
    "type of experiment. Then I want to motivate [contextual bandits](http://hunch.net/?p=298)\n",
    "as the further improved approach. Finally, I want to discuss another way to approach\n",
    "this study that involves already-collected data while still using bandit algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating some data\n",
    "\n",
    "I looked everywhere for data to use in the project. I would imagine between patient information and the difficulty of collecting data in an emergency medical situation, this data is not readily available. So let's make up some data that would most likely represent this scenario.\n",
    "\n",
    "We're going to create a class that outputs emergency events for us. It's over-simplified. The probability of survivability is fixed and is dependent on the type of emergency and the treatment administered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "emergency_type = ['stab', 'gunshot']\n",
    "\n",
    "class SimpleEmergency:\n",
    "    def __init__(self):\n",
    "        self.emergency_type = random.choice(emergency_type)\n",
    "        \n",
    "    def apply_treatment(self, treatment):\n",
    "        '''\n",
    "        A very simplified model of the emergency and the outcome based on treatment.\n",
    "        Outcome is treated as P(survive | (treatment, emergency_type))\n",
    "        \n",
    "        P( survive | (stab, basic)) = .6\n",
    "        P( survive | (stab, advanced)) = .5\n",
    "        P( survive | (gunshot, basic)) = .75\n",
    "        P( survive | (gunshot, advanced)) = .45\n",
    "        '''\n",
    "        \n",
    "        # If it was a stab wound and the basic treatment was applied.\n",
    "        if (self.emergency_type == 'stab') and (treatment == 'basic'):\n",
    "            if random.random() > .6:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        # If it was a stab wound and the advanced treatment was applied.\n",
    "        elif (self.emergency_type == 'stab') and (treatment == 'advanced'):\n",
    "            if random.random() > .5:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        # If it was a gunshot wound and the basic treatment was applied.\n",
    "        elif (self.emergency_type == 'gunshot') and (treatment == 'basic'):\n",
    "            if random.random() > .75:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        # If it was a gunshot wound and the advanced treatment was applied.\n",
    "        elif (self.emergency_type == 'gunshot') and (treatment == 'advanced'):\n",
    "            if random.random() > .45:\n",
    "                return True\n",
    "            else:\n",
    "                return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Bandit\n",
    "\n",
    "Now let's jump right into the bandit! We're going to be looking at one of the most popular bandit algorithms, the Upper Confidence Boundary (UCB) Bandit. Before we look at the code, let me give a description of the UCB bandit.\n",
    "\n",
    "A bandit explores and exploits. That means according to its policy, when presented with a trial from the environment, the agent, our dearly beloved bandit, will choose whether to try action and assess the received reward (_explore_) or it will choose the action that has been the most beneficial (_exploit_). You can generalize that bandits differ essentially by this policy for exploration vs. exploitation. The UCB bandit's policy will always choose the best arm that has historically given it the most reward __BUT__ it will try different arms if it hasn't tried them in a while.\n",
    "\n",
    "_\"How the hell does that work?\"_\n",
    "\n",
    "Well the UCB defines a function as follows:\n",
    "\n",
    "    action.value + sqrt((2 * log(total_counts)) / float(action.count))\n",
    "    ^              ^\n",
    "    |__ term 1     |______ term 2\n",
    "\n",
    "    - total_counts : number of times the bandit was faced with a decision and chose an arm.\n",
    "    - arm.count    : number of times this action has been chosen.\n",
    "    - arm.value    : historical average for reward received for this action.\n",
    "    \n",
    "### Term 1\n",
    "\n",
    "    action.value\n",
    "    \n",
    "As described above, __term 1__ is the historical average for the reward of this particular action.\n",
    "\n",
    "### Term 2\n",
    "\n",
    "    sqrt((2 * log(total_counts)) / float(action.count))\n",
    "    \n",
    "But __term 2__ does all the magic for exploration. __Term 2__ is a large number when `total_counts` is large and `action.count` is small. Interpreted, that means __term 2__ is large when a long period of time has passed without trying the action.\n",
    "\n",
    "But there's more to it!\n",
    "\n",
    "### All together now\n",
    "\n",
    "Together, __term 1__ and __term 2__ create a _value_ used by the bandit to choose which action to try. The bandit will always choose the action with the highest _value_. An action could have a low, historical reward (__term 1__) but if it's been long enough for __term 2__ to be large, the bandit will try that action. This has the very cool effect of trying poorer performing arms less!\n",
    "\n",
    "The UCB bandit will try poorer performing arms in the future. Is this a good thing or a bad thing. It depends on the environment. If this is a static environment, meaning there is an absolute best action for the bandit, then this is not an ideal feature of UCB. It will all try poorer performing arms though more infrequently as time progresses. If the environment is dynamic and the best decisions change over time, then this is a good feature. The bandit may try different arms and notice that the rewards are different. This in turn changes how it decides which arm to pick in the future.\n",
    "\n",
    "So finally, let's look at the entire code for the UCB bandit.\n",
    "[Code](https://github.com/johnmyleswhite/BanditsBook/blob/master/python/algorithms/ucb/ucb1.py) from the great John Myles White (@johnmyleswhite) whose [book is highly recommended](http://shop.oreilly.com/product/0636920027393.do) (and from which this slightly-edited code excerpt comes from)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def ind_max(x):\n",
    "  m = max(x)\n",
    "  return x.index(m)\n",
    "\n",
    "class UCB1():\n",
    "  def __init__(self, n_arms):\n",
    "    self.initialize(n_arms)\n",
    "    return\n",
    "  \n",
    "  def initialize(self, n_arms):\n",
    "    self.counts = [0 for col in range(n_arms)]\n",
    "    self.values = [0.0 for col in range(n_arms)]\n",
    "    return\n",
    "  \n",
    "  def select_arm(self):\n",
    "    n_arms = len(self.counts)\n",
    "    for arm in range(n_arms):\n",
    "      if self.counts[arm] == 0:\n",
    "        return arm\n",
    "\n",
    "    ucb_values = [0.0 for arm in range(n_arms)]\n",
    "    total_counts = sum(self.counts)\n",
    "    for arm in range(n_arms):\n",
    "      # IMPORTANT STUFF!\n",
    "      bonus = math.sqrt((2 * math.log(total_counts)) / float(self.counts[arm]))\n",
    "      ucb_values[arm] = self.values[arm] + bonus\n",
    "    return ind_max(ucb_values)\n",
    "  \n",
    "  def update(self, chosen_arm, reward):\n",
    "    self.counts[chosen_arm] = self.counts[chosen_arm] + 1\n",
    "    n = self.counts[chosen_arm]\n",
    "\n",
    "    value = self.values[chosen_arm]\n",
    "    new_value = ((n - 1) / float(n)) * value + (1 / float(n)) * reward\n",
    "    self.values[chosen_arm] = new_value\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Bandits\n",
    "\n",
    "As mentioned earlier, there are other bandits. They differ mostly in how their policy dictates exploration vs. exploitation. Here are some:\n",
    "\n",
    "* [UCB2](https://github.com/johnmyleswhite/BanditsBook/blob/master/python/algorithms/ucb/ucb2.py)\n",
    "* [Epsilon-Greedy](https://github.com/johnmyleswhite/BanditsBook/blob/master/python/algorithms/epsilon_greedy/standard.py)\n",
    "* [Epsilon-Greedy with Simulated Annealing](https://github.com/johnmyleswhite/BanditsBook/blob/master/python/algorithms/epsilon_greedy/annealing.py)\n",
    "* [EXP3](https://github.com/johnmyleswhite/BanditsBook/blob/master/python/algorithms/exp3/exp3.py)\n",
    "* [Hedge](https://github.com/johnmyleswhite/BanditsBook/blob/master/python/algorithms/hedge/hedge.py)\n",
    "* [Softmax](https://github.com/johnmyleswhite/BanditsBook/blob/master/python/algorithms/softmax/standard.py)\n",
    "* [Softmax with Simulated Annealing](https://github.com/johnmyleswhite/BanditsBook/blob/master/python/algorithms/softmax/annealing.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing our Bandit\n",
    "\n",
    "Let's test our bandit! The simulation will run as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_UCB1(trials, arm_labels):\n",
    "\n",
    "    # Initialize bandit with the number of actions it can perform.\n",
    "    bandit = UCB1(len(arm_labels))\n",
    "\n",
    "    for _ in range(trials):\n",
    "        # Create one emergency situation\n",
    "        e = SimpleEmergency()\n",
    "\n",
    "        # The bandit will select an action, an arm.\n",
    "        action = bandit.select_arm()\n",
    "\n",
    "        # See if the action, the treatment, is a success or not.\n",
    "        success = e.apply_treatment(arm_labels[action])\n",
    "\n",
    "        # If it's successful...\n",
    "        if success:\n",
    "            # We reward the action.\n",
    "            bandit.update(action, 1.0)\n",
    "        else:\n",
    "            # We don't reward the action.\n",
    "            bandit.update(action, 0.0)\n",
    "\n",
    "    return bandit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspection\n",
    "\n",
    "Let's inspect our bandit to see how it decides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "basic: 0.3064516129032258\n",
      "advanced: 0.5194063926940644\n"
     ]
    }
   ],
   "source": [
    "trials = 1000\n",
    "treatments = ['basic', 'advanced']\n",
    "\n",
    "bandit = train_UCB1(trials, treatments)\n",
    "\n",
    "for treatment, value in zip(treatments, bandit.values):\n",
    "    print('{}: {}'.format(treatment, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The larger number indicates the action that is best!\n",
    "\n",
    "Note: this result has no meaning on the Philadelphia experiments. Why? Because our data is made up! In fact, you should play with the percentages to see how it changes the outcome!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Bandits though?\n",
    "\n",
    "So we talked about bandits and how it can be used in this context. But why should we?\n",
    "\n",
    "Consider how the Philadelphia experiment is set-up. It's set-up much like how A/B testing sets up experiments: divide population in number of groups corresponding to the experimental parameters you would like to test. In this case, the population is divided in two, one for 'basic' treatment and the other for 'advanced' treatment.\n",
    "\n",
    "Let's make these differing approached face-off against one another! Let the most accurate win!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bandit_vs_AB():\n",
    "    \n",
    "    trials = 1000\n",
    "    treatments = ['basic', 'advanced']\n",
    "    bandit = UCB1(len(treatments))\n",
    "    bandit_score = 0\n",
    "    AB_score = 0\n",
    "\n",
    "    for i in range(trials):\n",
    "        # Create one emergency situation\n",
    "        e = SimpleEmergency()\n",
    "\n",
    "        # Bandit\n",
    "        action = bandit.select_arm()\n",
    "        bandit_treatment = treatments[action]\n",
    "        \n",
    "        bandit_success = e.apply_treatment(bandit_treatment)\n",
    "        if bandit_success:\n",
    "            bandit.update(action, 1.0)\n",
    "            bandit_score +=1\n",
    "        else:\n",
    "            bandit.update(action, 0.0)\n",
    "        \n",
    "        # A/B\n",
    "        if (i % 2) == 0:\n",
    "            AB_treatment = treatments[0]  # basic\n",
    "        else:\n",
    "            AB_treatment = treatments[1]  # advanced\n",
    "\n",
    "        AB_success = e.apply_treatment(AB_treatment)\n",
    "        if AB_success:\n",
    "            AB_score += 1\n",
    "\n",
    "    return bandit_score, AB_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the winner is..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bandit : 470\n",
      "A/B : 402\n"
     ]
    }
   ],
   "source": [
    "for setup, score in zip(['bandit', 'A/B'], bandit_vs_AB()):\n",
    "    print('{} : {}'.format(setup, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bandits win!!!\n",
    "\n",
    "## How?\n",
    "\n",
    "Bandit algorithms choose actions based on historical rewards per action. As it learns which action is the most fruitful, it chooses it more and more (I know I'm repeating myself at this point). This means we are able to benefit while learning about the experiment! So for Philadelphia, researchers can explore treatments early on in the experiment. But as time progresses, they start to get a sense of what works better and they start administering that action more and more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contextual Bandits\n",
    "\n",
    "During the progression of the argument for bandits, I'm sure most of you thought, \"this view is way too simplistic!\". You're right. In real life, there is much more information available than just 'gunshot' or 'stab'. In fact, some of you may have notice in the code that `select_arm()` doesn't event take an argument (for you all-star, snake-charming Pythonistas out there, yes, there is actually an argument, `self`, but you know what I mean :) )! That's the amazing thing about this bandits: it doesn't need a model of the environment! It proceeds solely off of the action-rewards interaction. But we're smart data scientists! We want to take advantage of all the information possibly available. __Enter contextual bandits.__\n",
    "\n",
    "Up until this point, our bandit only decides one action that applies to both 'stab' and 'gunshot' scenarios. But as many of you could imagine, it could vary. Perhaps 'basic' treatment works better for one or the other. In fact there may be other factors at play. Emergencies should actually look like...\n",
    "\n",
    "(note: at this point, the code is mainly to communicate my point. The fake data is starting to get really...fake)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "regions = ['head', 'torso', 'arm/leg']\n",
    "\n",
    "class RealEmergency:\n",
    "    def __init__(self):\n",
    "        self.emergency_type = random.choice(emergency_type)\n",
    "        self.age = random.randint(18, 65) \n",
    "        self.time_since_arrival = random.randint(60, 1080)  # seconds. Assume in Philly, all locations are within 18 min. of an ambulance\n",
    "        self.location_of_injury = random.choose(regions)\n",
    "        self.multiple_wounds = bool(random.randbits(1))\n",
    "        self.time_to_hospital = random.randint(60, 1080)  # seconds. Refer to \"time since arrival\" assumption.\n",
    "        # and possibly many others....\n",
    "        \n",
    "    def apply_treatment(self, treatment):\n",
    "        # The following is used to mock a distribution of survivability over the parameters above.\n",
    "        # IT IS NOT SCIENCE! It is only used for simulation!\n",
    "        \n",
    "        val = 0.0\n",
    "        if self.emergency_type == 'stab':\n",
    "            val = 3.42\n",
    "        else:\n",
    "            val = 9.6\n",
    "        if random.betavariate(\n",
    "            self.age/self.time_since_arrival + val,\n",
    "            self.location_of_injury + self.multiple_wounds + self.time_to_hospital\n",
    "        ) > .4:\n",
    "            return True\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given all this new information, how should the bandit deal with it?\n",
    "\n",
    "Well, when it decides which action to take, it should account for the context!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ContextualBandit():\n",
    "    def select_arm(self, context_vector):\n",
    "        # map context_vector to some qualitative value per arm.\n",
    "        # Let W_i be a 1-by-n vector where the context_vector is the column vector n-by-1.\n",
    "        # i is the arm index\n",
    "        \n",
    "        arm_values = [0.0 for arm in range(self.n_arms)]\n",
    "        for i in self.n_arms:\n",
    "             arm_values[i] = W_i[i] * context_vector  + calc_exploration_value(i) # we can add another term like in UCB too!\n",
    "                \n",
    "        return ind_max(arm_values)\n",
    "    \n",
    "    def update(self, context_vector, arm, reward):\n",
    "        # perform an update on the arm's historical reward.\n",
    "        update_arm(arm, reward)\n",
    "        \n",
    "        # Update our W row-vector\n",
    "        loss = compute_loss(context_vector, W_i[arm])\n",
    "        update_W(W_i[arm], loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the bandit above basically wraps a linear regression model to map the context to a reward.\n",
    "\n",
    "Remember, a bandit algorithm dictates a policy for exploration vs. exploitation. In a sense, bandit algorithms and reinforcement learning in general are just frameworks for learning. They are not disjoint from supervised and unsupervised learning.\n",
    "\n",
    "There are other contextual bandits. And they can be extremely complex. This one is actually considered pretty naive for a contextual bandit. I just want to show you how you take advantage of the context of a decision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Don't forget about bandit algorithms. Canonical examples for applications of bandit algorithms are health/medical experiments such as the ones being conducted in Philadelphia."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
